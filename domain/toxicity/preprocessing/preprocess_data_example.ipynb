{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Preprocessing Example: Toxicity\n",
    "\n",
    "This notebook contains preprocessing steps for setting up a text classification dataset for ENN.\n",
    "The example problem is Wikipedia toxic comment classification, i.e., toxicity.\n",
    "These steps serve as a reference for applying ENN to other text classification problems.\n",
    "\n",
    "WARNING: Please be aware that the toxicity dataset contains many highly-offensive text comments, which may be encountered when handling the dataset. The inclusion of such offensive text is intentional. Part of the goal of the problem is to identify such innapropriate and unhelpful content.\n",
    "\n",
    "Note: To follow this notebook interactively, jupyter can be run locally or on a remote machine. General documentation for jupyter can be found here: https://jupyter-notebook.readthedocs.io/en/stable/; and sample instructions for running remotely can be found here: https://towardsdatascience.com/running-jupyter-notebooks-on-remote-servers-603fbcc256b3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data\n",
    "\n",
    "This assumes the data is in a three-column tsv file, as in `toxicity-raw-data-sample.tsv`.\n",
    "It is tsv instead of csv because the comments may include commas, but not tabs.\n",
    "The file has the following columns:\n",
    "- 'label': the class label of the sample, i.e., 0 for non-toxic, 1 for toxic.\n",
    "- 'text': the raw text of the Wikipedia comment.\n",
    "- 'split': the data split of the sample, either 'train', 'dev' (i.e., for validation), or 'test'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s Kamen Rider. Ronhjones</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>No way, Mercedez PWNZ!!</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>MoP, Have you been reading anything I have w...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>== Country profiles? ==  Hi, I saw your noti...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Pete, I have discussed this change. You are ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  split\n",
       "0      0                           s Kamen Rider. Ronhjones  train\n",
       "1      0                            No way, Mercedez PWNZ!!  train\n",
       "2      0    MoP, Have you been reading anything I have w...    dev\n",
       "3      0    == Country profiles? ==  Hi, I saw your noti...   test\n",
       "4      0    Pete, I have discussed this change. You are ...   test"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data_file = 'toxicity-raw-data-sample.tsv'\n",
    "raw_df = pd.read_csv(raw_data_file, sep='\\t')\n",
    "raw_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "This step separates the data into input (text) and output (labels), for each of the three specified splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0, 0, 0, 0, 0]\n",
      "Text: ['s Kamen Rider. Ronhjones', '  No way, Mercedez PWNZ!!', \"  == Re: Coordinator Elections! ==  Well, while I've been a bit tardy these recent times, yes, I will probably stand again now that the pressures of life are easing off! I'm also going to put in some graft and finish the last chunks of my pet project, Military history of New Zealand. Good luck to you!  e \", '  == February 2006 ==  Thanks for experimenting with the page Arminianism on Wikipedia. Your test worked, and has been reverted or removed. Please use the sandbox for any other tests you want to do. Take a look at the welcome page if you would like to learn more about contributing to our encyclopedia.  Thanks.   ', '  what european power took over algeria?']\n"
     ]
    }
   ],
   "source": [
    "splits = ['train', 'test', 'dev']\n",
    "split_labels = {}\n",
    "raw_split_text = {}\n",
    "for split in splits:\n",
    "    labels = raw_df.loc[raw_df['split'] == split]['label'].tolist()\n",
    "    text = raw_df.loc[raw_df['split'] == split]['text'].tolist()\n",
    "    split_labels[split] = labels\n",
    "    raw_split_text[split] = text\n",
    "print(\"Labels:\", split_labels['train'][:5])\n",
    "print(\"Text:\", raw_split_text['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text\n",
    "This step creates a mapping between words (or \"tokens\") and integers, and uses this mapping to encode each text sample as a sequence that can be fed into a Keras model.\n",
    "\n",
    "This step uses a parameter `vocab_size`, which specifies how many unique words will be in the mapping. Words are preferred by frequency, and words not in the mapping are discarded, i.e., they are deemed too rare to lead to useful generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[235], [43, 145], [429, 117, 183, 139, 59, 7, 322, 85, 368, 236, 268, 6, 68, 369, 108, 86, 10, 1, 3, 430, 24, 161, 73, 54, 162, 2, 223, 5, 53, 4, 1, 293, 3, 33, 224, 269, 294, 431, 3, 118, 129, 836, 2, 8, 323], [432, 837, 87, 14, 25, 1, 35, 17, 40, 27, 838, 839, 4, 56, 59, 645, 28, 295, 67, 119, 1, 646, 14, 82, 65, 8, 124, 2, 32, 163, 7, 134, 39, 1, 515, 35, 34, 8, 48, 58, 2, 51, 42, 2, 164, 370, 87], [37, 840, 647, 225]]\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "vocab_size = 1000\n",
    "tokenizer = Tokenizer(num_words=vocab_size)                                              \n",
    "tokenizer.fit_on_texts(raw_split_text['train'])\n",
    "tok_split_text = {}\n",
    "for split in splits:\n",
    "    tokenized = tokenizer.texts_to_sequences(raw_split_text[split])\n",
    "    tok_split_text[split] = tokenized\n",
    "print(tok_split_text['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data\n",
    "Use pickle to save the preprocessed data into datafiles, which can then be used by ENN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('toxicity_labels_example.pkl', 'wb') as f:\n",
    "    pickle.dump(split_labels, f)\n",
    "with open('toxicity_tokens_example.pkl'.format(vocab_size), 'wb') as f:\n",
    "    pickle.dump(tok_split_text, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Preprocess pretrained embeddings\n",
    "\n",
    "The toxicity domain includes the option of using pre-trained word embeddings.\n",
    "Embeddings map each word to a fixed size vector of real numbers that encodes prior knowledge of the word's meaning.\n",
    "ENN may find such embeddings to be useful during the evolutionary process.\n",
    "Such embeddings are trained on huge corpuses of text, so can be useful especially for establishing word meaning in applications where there is not very much data for the problem.\n",
    "\n",
    "Raw pre-trained embeddings files can be very large (multiple gigs), since they may contain embeddings for millions of words.\n",
    "So, it is useful to preprocess them once, so that ENN does need to handle repeatedly on each worker.\n",
    "Preprocessed embeddings can then be accessed in ENN as additional data files.\n",
    "Common pre-trained embeddings types include word2vec, fasttext, and glove.\n",
    "These are available for download at various locations. E.g.,\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://fasttext.cc/\n",
    "\n",
    "The step for loading these raw files may need to be adjusted for embedding files in alternate formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_file = 'glove.6B.50d.txt'\n",
    "\n",
    "# Load pre-trained embeddings file\n",
    "def load_embedding(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embedding_dictionary = dict(load_embedding(*o.strip().split(\" \"))\n",
    "                       for o in open(embeddings_file))\n",
    "print(embedding_dictionary['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings matrix\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index))\n",
    "embedding_size = len(embedding_dictionary['cat'])\n",
    "embedding_matrix = np.zeros((nb_words, embedding_size))\n",
    "for word, i in list(word_index.items()):\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embedding_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix[word_index['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embedding matrix to file\n",
    "embeddings_matrix_filename = 'embeddings_matrix.pkl'.format(vocab_size)\n",
    "with open(embeddings_matrix_filename, 'wb') as f:\n",
    "    pickle.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
